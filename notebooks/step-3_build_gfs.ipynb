{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "from pyarabic.araby import DIACRITICS, SHADDA, LETTERS, is_arabicword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last Harakat from an Arabic Word\n",
    "NOT_LETTERS_PATTERN = f\"[^{LETTERS}]\"\n",
    "DIACRITICS_PATTERN = \"\".join(DIACRITICS)\n",
    "LAST_HARAKAT_PATTERN = re.compile(\n",
    "    rf\"[{DIACRITICS_PATTERN}](?={NOT_LETTERS_PATTERN}*$)\", re.UNICODE\n",
    ")\n",
    "\n",
    "MORPHOLOGY_MAP = {\n",
    "    \"gender\": \"g\",\n",
    "    \"root\": \"root\",\n",
    "    \"plural\": \"pl\",\n",
    "    \"masc_pl\": \"masc_pl\",\n",
    "    \"fem_pl\": \"fem_pl\",\n",
    "    \"imperfect\": \"imperfect\",\n",
    "    \"verb_form\": \"cls\",\n",
    "}\n",
    "\n",
    "# Set of Harakat\n",
    "DIACRITICS_SET = set(DIACRITICS)  # Aabic diacritics/short vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/interim/lexicon\")\n",
    "ar_adjectives_path = data_dir / \"20231028.172908_adjectives_lexicon.csv\"\n",
    "ar_verbs_path = data_dir / \"20231028.172908_verbs_lexicon.csv\"\n",
    "ar_nouns_path = data_dir / \"20231028.172908_nouns_lexicon.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_shadda(ar_string: str) -> str:\n",
    "    \"\"\"unicodedata.normalize put shadda before diacritics; not correct\"\"\"\n",
    "    list_ar_str = list(ar_string)\n",
    "\n",
    "    for i in range(len(list_ar_str) - 1):\n",
    "        char = list_ar_str[i]\n",
    "        next_char = list_ar_str[i + 1]\n",
    "\n",
    "        if char in DIACRITICS_SET and next_char == SHADDA:\n",
    "            list_ar_str[i], list_ar_str[i + 1] = (\n",
    "                next_char,\n",
    "                char,\n",
    "            )  # Swap shadda and diacritic\n",
    "\n",
    "    return \"\".join(list_ar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ar(ar_vocalized: str, verbose: bool = False) -> str:\n",
    "    \"\"\"get the normal form for the Unicode string unistr using NFC then fix the shadda order issue\"\"\"\n",
    "    ar_norm = normalize(\"NFC\", ar_vocalized)\n",
    "    ar_norm = reorder_shadda(ar_norm)\n",
    "    if verbose:\n",
    "        print([name(char) for char in ar_norm])\n",
    "    return ar_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lin(row):\n",
    "    row_dict = dict(row)\n",
    "    list_lins = []\n",
    "    for morpho in row_dict:\n",
    "        if lin_feature := MORPHOLOGY_MAP.get(morpho):\n",
    "            lin_value = row_dict[morpho]\n",
    "            if is_arabicword(lin_value):\n",
    "                lin_value = f'\"{lin_value}\"'\n",
    "            list_lins.append(f'{lin_feature} = {lin_value}')\n",
    "    return list_lins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gf_abstract_entries(row):\n",
    "\n",
    "    cat = row[\"en\"].split(\"_\")[-1][0]\n",
    "    lemma = normalize_ar(row[\"vocal_forms\"])\n",
    "    lemma = LAST_HARAKAT_PATTERN.sub(\"\", lemma)\n",
    "    idx = row[\"wiki_idx\"]\n",
    "    senses = row[\"senses\"]\n",
    "    source = \"wikitionary\"\n",
    "\n",
    "    gf_fun_str = \"fun {}_{} : {} ; \"\n",
    "    comment_str = \"-- source: {}, idx: {}, senses: {}\"\n",
    "\n",
    "    gf_fun_str = gf_fun_str.format(lemma, cat, cat)\n",
    "    comment_str = comment_str.format(source, idx, senses)\n",
    "\n",
    "    list_lins = get_lin(row)\n",
    "    if cat == \"V\":\n",
    "        list_lins.append(f'perfect = \"{lemma}\"')\n",
    "    elif cat == \"N\":\n",
    "        list_lins.append(f'sg = \"{lemma}\"')\n",
    "    elif cat == \"adj\":\n",
    "        if row[\"gender\"] == \"masc\":\n",
    "            list_lins.append(f'masc_sg = \"{lemma}\"')\n",
    "            lemma_otherg = normalize_ar(row[\"other_gender_form\"])\n",
    "            lemma_otherg = LAST_HARAKAT_PATTERN.sub(\"\", lemma_otherg)\n",
    "            list_lins.append(f'fem_sg = \"{lemma_otherg}\"')\n",
    "        else:\n",
    "            list_lins.append(f'fem_sg = \"{lemma}\"')\n",
    "            lemma_otherg = normalize_ar(row[\"other_gender_form\"])\n",
    "            lemma_otherg = LAST_HARAKAT_PATTERN.sub(\"\", lemma_otherg)\n",
    "            list_lins.append(f'masc_sg = \"{lemma_otherg}\"')\n",
    "\n",
    "    str_lins = \" ; \".join(list_lins)\n",
    "    lin_entry = f\"'{lemma}_{cat}'\"\n",
    "    lin = f\"lin {lin_entry} = wmk{cat} { {str_lins} } ;\"\n",
    "\n",
    "    return f\"{gf_fun_str}{comment_str}\", lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjs = pd.read_csv(ar_adjectives_path, index_col=0, converters={\"senses\": pd.eval})\n",
    "df_nouns = pd.read_csv(ar_nouns_path, index_col=0, converters={\"senses\": pd.eval})\n",
    "df_verbs = pd.read_csv(ar_verbs_path, index_col=0, converters={\"senses\": pd.eval})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Abstract GF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_adjs[\u001b[39m\"\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m\"\u001b[39m], df_adjs[\u001b[39m\"\u001b[39m\u001b[39mcnc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_adjs\u001b[39m.\u001b[39;49mapply(build_gf_abstract_entries, axis\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m df_nouns[\u001b[39m\"\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m\"\u001b[39m], df_nouns[\u001b[39m\"\u001b[39m\u001b[39mcnc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_nouns\u001b[39m.\u001b[39mapply(build_gf_abstract_entries, axis\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m df_verbs[\u001b[39m\"\u001b[39m\u001b[39mimperfect\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_verbs\u001b[39m.\u001b[39mapply({\u001b[39m\"\u001b[39m\u001b[39mimperfect\u001b[39m\u001b[39m\"\u001b[39m: normalize_ar})\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/thss-m/lib/python3.9/site-packages/pandas/core/frame.py:9433\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9422\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9424\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9425\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9426\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9431\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9432\u001b[0m )\n\u001b[0;32m-> 9433\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/thss-m/lib/python3.9/site-packages/pandas/core/apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/thss-m/lib/python3.9/site-packages/pandas/core/apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 798\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    800\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/thss-m/lib/python3.9/site-packages/pandas/core/apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    812\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    813\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    815\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    816\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    817\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m str_lins \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m ; \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(list_lins)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m lin_entry \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlemma\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcat\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m lin \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlin \u001b[39m\u001b[39m{\u001b[39;00mlin_entry\u001b[39m}\u001b[39;00m\u001b[39m = wmk\u001b[39m\u001b[39m{\u001b[39;00mcat\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m \u001b[39m{{str_lins}}\u001b[39m \u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ;\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zarzouram/workspace/MLT_Thesis/notebooks/step-3_build_gfs.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mgf_fun_str\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mcomment_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, lin\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'set'"
     ]
    }
   ],
   "source": [
    "df_adjs[\"abs\"], df_adjs[\"cnc\"] = df_adjs.apply(build_gf_abstract_entries, axis=\"columns\")\n",
    "df_nouns[\"abs\"], df_nouns[\"cnc\"] = df_nouns.apply(build_gf_abstract_entries, axis=\"columns\")\n",
    "\n",
    "df_verbs[\"imperfect\"] = df_verbs.apply({\"imperfect\": normalize_ar})\n",
    "df_verbs[\"imperfect\"] = df_verbs.apply({\"imperfect\": lambda s: LAST_HARAKAT_PATTERN.sub(\"\", s)})\n",
    "df_verbs[\"abs\"], df_nouns[\"cnc\"] = df_verbs.apply(build_gf_abstract_entries, axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thss-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
