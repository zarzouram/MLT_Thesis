{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Wikitionary Dump File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from unicodedata import normalize\n",
    "\n",
    "from pyarabic.araby import DIACRITICS, SHADDA, name\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip_inpath = \"../../data/raw/wikidata/raw-wiktextract-data.json.gz\"\n",
    "gzip_outpath = \"../../data/processed/wikidata/ar-wiktextract-data.json.gz\"\n",
    "gzip_reindexedpath = \"../../data/processed/wikidata/ar_reindex.json.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIACRITICS = set(DIACRITICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_shadda(ar_string: List[str]) -> str:\n",
    "    \"\"\"unicodedata.normalize put shadda before diacritics\"\"\"\n",
    "    list_ar_str = list(ar_string)\n",
    "\n",
    "    for i in range(len(list_ar_str) - 1):\n",
    "        char = list_ar_str[i]\n",
    "        next_char = list_ar_str[i + 1]\n",
    "\n",
    "        if char == SHADDA and next_char in DIACRITICS:\n",
    "            list_ar_str[i], list_ar_str[i + 1] = (\n",
    "                next_char,\n",
    "                char,\n",
    "            )  # Swap shadda and diacritic\n",
    "\n",
    "    return \"\".join(list_ar_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ar(ar_vocalized: str, verbose: bool = False) -> str:\n",
    "    ar_norm = normalize(\"NFC\", ar_vocalized)\n",
    "    ar_norm = reorder_shadda(ar_norm)\n",
    "    if verbose:\n",
    "        print([name(char) for char in ar_norm])\n",
    "    return ar_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dump File and Reindexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download dump file from [here](https://kaikki.org/dictionary/rawdata.html)\\\n",
    "  Thanks: Tatu Ylonen: Wiktextract: Wiktionary as Machine-Readable Structured Data,\\\n",
    "  Proceedings of the 13th Conference on Language Resources and Evaluation (LREC), pp. 1317-1325, Marseille, 20-25 June 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load raw wikitionary dump file:\n",
    "  - Extract Arabic words\n",
    "  - Reindexing `json` object to be keyed with the Arabic words\n",
    "  - Save the line number for easier acess in future for each word\n",
    "    - Words can be repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wikipedia', 'instances', 'synonyms', 'source', 'antonyms', 'hyponyms', 'redirect', 'proverbs', 'related', 'troponyms', 'title', 'holonyms', 'abbreviations', 'translations', 'coordinate_terms', 'form_of', 'head_templates', 'pos', 'categories', 'redirects', 'inflection_templates', 'sounds', 'hyphenation', 'etymology_text', 'descendants', 'derived', 'senses', 'lang', 'forms', 'topics', 'lang_code', 'meronyms', 'wikidata', 'etymology_number', 'word', 'alt_of', 'etymology_templates', 'hypernyms'}\n"
     ]
    }
   ],
   "source": [
    "# Process the data and gather it in a list\n",
    "data_to_write = []\n",
    "words_reindexed = defaultdict(list)\n",
    "indx = 0\n",
    "\n",
    "with gzip.open(gzip_inpath, \"rt\", encoding=\"utf-8\") as gzip_inobj:\n",
    "    for i, line in enumerate(gzip_inobj):\n",
    "        inobj = json.loads(line)\n",
    "        if inobj.get(\"lang_code\", \"\") == \"ar\" and inobj.get(\"word\"):\n",
    "            word = inobj[\"word\"]\n",
    "            ar_dict = {k: v for k, v in inobj.items() if k != \"word\"}\n",
    "            word = normalize_ar(word)\n",
    "            words_reindexed[word].append(indx)\n",
    "            indx += 1\n",
    "\n",
    "            # Serialize the new dictionary structure and add it to the list\n",
    "            data_to_write.append(json.dumps({word: ar_dict}))\n",
    "        if (i % 999) == 0:\n",
    "            print(f\"Reading Line: {i}\", end=\"\\r\")\n",
    "\n",
    "# Write the list to the GZIP file\n",
    "# with gzip.open(gzip_outpath, \"wt\", encoding=\"utf-8\") as gzip_outobj:\n",
    "#     for entry in tqdm(data_to_write):\n",
    "#         gzip_outobj.write(entry + \"\\n\")\n",
    "\n",
    "# # Serialize and write the reindex data\n",
    "# with gzip.open(gzip_reindexedpath, \"wt\", encoding=\"utf-8\") as gzip_obj:\n",
    "#     json_reindex = json.dumps(words_reindexed)\n",
    "#     gzip_obj.write(json_reindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the reindexed files size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26M\t../data/processed/wikidata/ar-wiktextract-data.json.gz\n",
      "556K\t../data/processed/wikidata/ar_reindex.json.gz\n"
     ]
    }
   ],
   "source": [
    "op_dir = f'{\"/\".join(gzip_outpath.split(\"/\")[:-1])}/*'\n",
    "!du -h {op_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the produced files are small, they could be loaded to the PC RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thss-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
