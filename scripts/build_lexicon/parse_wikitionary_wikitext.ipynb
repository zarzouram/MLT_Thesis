{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import requests\n",
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from math import ceil\n",
    "import random\n",
    "from statsmodels.stats.proportion import samplesize_confint_proportion\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from ism_ar import Ism, IsmDict\n",
    "from utilities import remove_diacritics_ar, Status\n",
    "\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reading Arabic Conll-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../../outputs/lexicons_ar/asmaa.pkl\"\n",
    "results_dir = \"../../outputs/lexicons_ar/wikitionary_ar/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, 'rb') as f:\n",
    "    nouns, adjs, xs, sus_plurals = pickle.load(f)\n",
    "    nouns: IsmDict\n",
    "    adjs: IsmDict\n",
    "    xs: IsmDict\n",
    "    sus_plurals: IsmDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Wikitinary Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entry_data(entry: str):\n",
    "    \"\"\"\n",
    "    Retrieves parsed wikitext data for an Arabic word from the English\n",
    "    Wiktionary API.\n",
    "\n",
    "    Args:\n",
    "        entry: A string representing the Arabic word to look up.\n",
    "\n",
    "    Returns:\n",
    "        The JSON response from the Wiktionary API, as a Python dictionary, if\n",
    "        the request is successful.\n",
    "        False if the request fails after three attempts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the API endpoint URL, which includes a placeholder for the word to\n",
    "    # look up.\n",
    "    url = 'https://en.wiktionary.org/w/api.php?action=parse&page={}&prop=wikitext&formatversion=2&format=json'  # noqa: E501\n",
    "\n",
    "    # Make three attempts to make a GET request to the API with the word to\n",
    "    # look up. If any of the requests succeeds and returns valid JSON data,\n",
    "    # return it. If all three requests fail (due to a network error\n",
    "    # or an exception), return False to indicate that the request failed.\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            r = requests.get(url.format(entry), timeout=60).json()\n",
    "            return r\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_data(ism: Ism) -> Tuple[Status, Union[str, None], Ism]:\n",
    "    \"\"\"\n",
    "    Retrieves parsed data for an Arabic word, given an Ism object.\n",
    "\n",
    "    Args:\n",
    "        ism: An Ism object that represents the Arabic word to look up.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with three elements:\n",
    "            - A Status object that represents the status of the request.\n",
    "            - A dictionary containing the parsed wikitext data for the word, if\n",
    "            it was found.\n",
    "            - The original Ism object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a message to use when a page is not found.\n",
    "    NOT_FOUND_MSG = \"The page you specified doesn't exist.\"\n",
    "    arabic_regex = re.compile(r'==Arabic==\\n.+?(?===\\w+==\\n|<<END>>)',\n",
    "                              flags=re.S)\n",
    "\n",
    "    # If the Ism object does not have the respective pliral form, look up the\n",
    "    # word.\n",
    "    if not ism.plural:\n",
    "        # Normalize the word by removing diacritics.\n",
    "        lemma = ism.lemma\n",
    "        lemma_nodiac = remove_diacritics_ar(lemma)\n",
    "\n",
    "        # Look up the word by using its lemma in the English Wiktionary.\n",
    "        data = get_entry_data(lemma_nodiac)\n",
    "\n",
    "        # If the word was found, retrieve its parsed wikitext data.\n",
    "        if data:\n",
    "\n",
    "            # the outputcould be the parsed wikitext or an error message.\n",
    "            # If the parsed data exists and contains wikitext, return it.\n",
    "            parsed = data.get('parse')\n",
    "            if parsed:\n",
    "                if parsed['wikitext']:\n",
    "                    wikitext = parsed['wikitext'] + '\\n<<END>>\\n'\n",
    "                    wikitext_ar = arabic_regex.search(wikitext)\n",
    "                    if wikitext_ar is None:\n",
    "                        return Status.NoArabic, None, ism\n",
    "                    return Status.EntryFound, wikitext_ar.group(), ism\n",
    "\n",
    "            # retrieving parsing wikitext was not sucssefull. Try to look by\n",
    "            # the word form itself. Most probabily the lemma was wrong.\n",
    "            else:\n",
    "                # tru again using word form removing definite article prefix\n",
    "                # (al-)\n",
    "                word = remove_diacritics_ar(ism.form)\n",
    "                word = re.sub('^ال', \"\", word)\n",
    "                data = get_entry_data(word)\n",
    "\n",
    "                # If the parsed data does not exist, check the error message\n",
    "                if data:\n",
    "                    # If the word was found and its parsed data contains\n",
    "                    # wikitext, return it.\n",
    "                    parsed = data.get('parse')\n",
    "                    if parsed:\n",
    "                        if parsed['wikitext']:\n",
    "                            wikitext = parsed['wikitext'] + '\\n<<END>>\\n'\n",
    "                            wikitext_ar = arabic_regex.search(wikitext)\n",
    "                            if wikitext_ar is None:\n",
    "                                return Status.NoArabic, None, ism\n",
    "                            return Status.EntryFound, wikitext_ar.group(), ism\n",
    "\n",
    "                    # If the parsed data does not exist, check the error\n",
    "                    # message\n",
    "                    else:\n",
    "                        error = data.get('error')\n",
    "                        if error:\n",
    "                            error_msg: str = error['info']\n",
    "                            if error_msg == NOT_FOUND_MSG:\n",
    "                                return Status.EntryNotFound, None, ism\n",
    "                            elif error_msg.startswith('Bad title'):\n",
    "                                return Status.BadTitle, None, ism\n",
    "                            elif error_msg.startswith('There is no section 1'):\n",
    "                                return Status.NoArabic, None, ism\n",
    "\n",
    "                            # error is unknown. Get error messege.\n",
    "                            else:\n",
    "                                msg = \"_\".join(error_msg.split())\n",
    "                                Status.add_error(msg)\n",
    "                                status = getattr(Status, msg)\n",
    "                                return status, None, ism\n",
    "\n",
    "                        # the data returned from API does not contains the\n",
    "                        # parsed data or an error messege. Return unknown\n",
    "                        # error.\n",
    "                        return Status.UNKOWN, None, ism\n",
    "\n",
    "        # API request was not sucssefull.\n",
    "        return Status.RequestException, None, ism\n",
    "\n",
    "    # Plural is aready exists, no need to use Wikitionary.\n",
    "    else:\n",
    "        return Status.PluralExist, None, ism\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Wikitionary Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Nouns Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8130d42fda14817bc2debd6d6eb8e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Pool(processes=15) as pool:\n",
    "    noun_parsed = list(\n",
    "        tqdm(pool.imap(get_parsed_data, nouns), total=len(nouns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({<Status.EntryFound: 1>: 1928,\n",
      "         <Status.EntryNotFound: 2>: 445,\n",
      "         <Status.PluralExist: 4>: 240,\n",
      "         <Status.NoArabic: 7>: 41})\n"
     ]
    }
   ],
   "source": [
    "statuses, parsed, isms = zip(*noun_parsed)\n",
    "statuses_counter = Counter(statuses)\n",
    "pprint(statuses_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{results_dir}nouns_wikitionary'\n",
    "with open(f'{file_name}.pkl', \"wb\") as f:\n",
    "    pickle.dump(noun_parsed, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6a5a402e694ff98b8e878edce539de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Pool(processes=15) as pool:\n",
    "    adj_parsed = list(tqdm(pool.imap(get_parsed_data, adjs), total=len(adjs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({<Status.EntryFound: 1>: 943,\n",
      "         <Status.EntryNotFound: 2>: 532,\n",
      "         <Status.PluralExist: 4>: 49,\n",
      "         <Status.NoArabic: 7>: 21})\n"
     ]
    }
   ],
   "source": [
    "statuses, parsed, isms = zip(*adj_parsed)\n",
    "pprint(Counter(statuses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{results_dir}adjectives_wikitionary'\n",
    "with open(f'{file_name}.pkl', \"wb\") as f:\n",
    "    pickle.dump(adj_parsed, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a16ca5a402442e899af37e91b000c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Pool(processes=15) as pool:\n",
    "    x_parsed = list(tqdm(pool.imap(get_parsed_data, xs), total=len(xs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({<Status.EntryNotFound: 2>: 2190,\n",
      "         <Status.EntryFound: 1>: 659,\n",
      "         <Status.NoArabic: 7>: 91})\n"
     ]
    }
   ],
   "source": [
    "statuses, parsed, isms = zip(*x_parsed)\n",
    "pprint(Counter(statuses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'{results_dir}X_wikitionary'\n",
    "with open(f'{file_name}.pkl', \"wb\") as f:\n",
    "    pickle.dump(x_parsed, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thss-m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
